# Base Python image (CPU)
FROM python:3.10-slim

# Install build tools and dependencies
RUN apt-get update && apt-get install -y \
    git cmake build-essential curl \
    libopenblas-dev libsqlite3-dev libssl-dev libcurl4-openssl-dev pkg-config \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /llama

# Clone llama.cpp
RUN git clone --depth=1 https://github.com/ggerganov/llama.cpp.git .

# Build llama.cpp server (CPU only)
RUN mkdir -p build && cd build && \
    cmake .. \
        -DGGML_CUDA=off \
        -DLLAMA_BUILD_SERVER=on \
        -DLLAMA_BUILD_EXAMPLES=off \
        -DLLAMA_BUILD_TESTS=off && \
    cmake --build . --config Release -j"$(nproc)" && \
    # Verify server binary
    ls -lah bin && \
    if [ -x bin/server ]; then echo "server binary OK"; \
    elif [ -x bin/llama-server ]; then ln -s llama-server bin/server; \
    else echo "ERROR: server binary not found" && exit 1; fi

# Expose server port
EXPOSE 8000

# Start server with CPU

CMD sh -c "/llama/build/bin/server -m $MODEL_PATH --port 8000 --host 0.0.0.0 -c 1024 -b 128 -ub 32"

