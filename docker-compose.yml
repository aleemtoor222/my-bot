version: "3.9"

services:
  llama-server:
    build:
      context: .
      dockerfile: Dockerfile.model
    container_name: llama-server
    volumes:
      - ./models:/models
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu
    runtime: nvidia   # Explicitly use NVIDIA runtime

  hrms-app:
    build:
      context: .
      dockerfile: Dockerfile.app
    container_name: hrms-app
    volumes:
      - .:/app
    ports:
      - "8070:8070"
    depends_on:
      - llama-server
    environment:
      - MODEL_ENDPOINT=http://llama-server:8000/completion
